---
term: "Hallusinasie"
slug: "hallusinasie"
shortDefinition: "Wanneer 'n KI-stelsel oortuigend verkeerde of verdigte inligting as feite aanbied."
related: ["groot-taalmodel-llm", "rag"]
tags: ["belangrik", "risiko"]
---

# Hallusinasie

'n **Hallusinasie** in KI-konteks is wanneer 'n model oortuigend verkeerde, nie-bestaande of verdigte inligting genereer asof dit 'n feit is.

## Eenvoudige verduideliking

KI-modelle "weet" nie regtig nie - hulle voorspel waarskynlike woorde. Soms is die mees waarskynlike antwoord heeltemal verkeerd, maar die model skryf dit met volle selfvertroue neer.

## Voorbeelde

- 'n KI noem 'n boek wat nie bestaan nie
- Dit verwys na 'n wetenskaplike studie wat verdig is
- Dit gee 'n biografiese feit wat heeltemal foutief is
- Dit haal 'n bron aan wat nooit bestaan het nie

## Hoekom gebeur dit?

1. **Patroonpassing** - die model soek na wat "reg klink" meer as wat werklik waar is
2. **Ontbrekende inligting** - as die model nie die antwoord ken nie, vul dit die gaping
3. **Druk om te antwoord** - die model is opgelei om antwoorde te gee, nie om "ek weet nie" te sÃª nie

## Hoe om jouself te beskerm

- **Verifieer altyd** feite teen betroubare bronne
- **Wees skepties** teenoor spesifieke aanhalings, datums of statistieke
- **Vra vir bronne** - en kontroleer of hulle werklik bestaan
- **Gebruik gesonde verstand** - as iets snaaks klink, ondersoek verder

## Die belangrike les

'n KI se selfvertroue het geen verband met akkuraatheid nie. Dit kan met absolute sekerheid heeltemal verkeerd wees.
